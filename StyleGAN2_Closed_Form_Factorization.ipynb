{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2-Closed-Form-Factorization.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipecalegario/stylegan2-ada-experiments/blob/main/StyleGAN2_Closed_Form_Factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx0yRqdfsxOW"
      },
      "source": [
        "# Week 3: Feature Vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHTmbclksrJy"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "!pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTCz_oDnwamg"
      },
      "source": [
        "!wget http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-cat-config-f.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef14ezRWv1PU"
      },
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import cv2\n",
        "import IPython.display\n",
        "import dnnlib\n",
        "import torch\n",
        "\n",
        "import legacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uesuziPRwYcN"
      },
      "source": [
        "network_pkl = '/content/drive/MyDrive/GDL-studies/ERN-FUFI/modelo/ernesto-inicia-network-snapshot-000016.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQbW3UYwebA"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcI3iREJWSpt"
      },
      "source": [
        "## Generate Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icQicrT2wJdv"
      },
      "source": [
        "modulate = {\n",
        "    k[0]: k[1]\n",
        "    for k in G.named_parameters()\n",
        "    if \"affine\" in k[0] and \"torgb\" not in k[0] and \"weight\" in k[0] or (\"torgb\" in k[0] and \"b4\" in k[0] and \"weight\" in k[0] and \"affine\" in k[0])\n",
        "}\n",
        "\n",
        "weight_mat = []\n",
        "for k, v in modulate.items():\n",
        "    weight_mat.append(v)\n",
        "\n",
        "W = torch.cat(weight_mat, 0)\n",
        "eigvec = torch.linalg.svd(W).Vh.to(\"cpu\")\n",
        "\n",
        "torch.save({\"ckpt\": network_pkl, \"eigvec\": eigvec}, '/content/drive/MyDrive/GDL-studies/ERN-FUFI/modelo/ern-feature-vectors.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW2J4DclyFPH"
      },
      "source": [
        "print(eigvec.shape) # how many dimensions\n",
        "print(eigvec[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA9uui5YW5lU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MQ5-xGKrBbB"
      },
      "source": [
        "## Applying a feature vector to an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3jljMlVttVR"
      },
      "source": [
        "def imshow(images, col, viz_size=1024):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  num, height, width, channels = images.shape\n",
        "  assert num % col == 0\n",
        "  row = num // col\n",
        "  # print(num,height,width,channels)\n",
        "\n",
        "  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n",
        "\n",
        "  for idx, image in enumerate(images):\n",
        "    i, j = divmod(idx, col)\n",
        "    y = i * viz_size\n",
        "    x = j * viz_size\n",
        "    if height != viz_size or width != viz_size:\n",
        "      image = cv2.resize(image, (viz_size, viz_size))\n",
        "    fused_image[y:y + viz_size, x:x + viz_size] = image\n",
        "\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  im_data = data.getvalue()\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg6YMt1BrMOb"
      },
      "source": [
        "seed = 3923\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "\n",
        "truncation_psi = 1.0\n",
        "noise_mode = 'const' # 'const', 'random', 'none'\n",
        "\n",
        "outdir = '/content/output/'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "# no labels\n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "\n",
        "z = torch.from_numpy(z).to(device)\n",
        "img_gpu = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "img = (img_gpu.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n",
        "\n",
        "imshow(img.cpu(),col=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWUVEXwgtHIk"
      },
      "source": [
        "You’ll see something referred to as `degree` when discussing vectors. It’s probably easier to think of this as strength. Strength can be a negative or positive value, and the large the value (in either direction) the stronger the effect on the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoyvYLD5YKvx"
      },
      "source": [
        "0.5, 0.5 (-0.01*1000,0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJhVpsN1rXfy"
      },
      "source": [
        "seed = 3923\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "z = torch.from_numpy(z)\n",
        "\n",
        "degree = 10.0 # we'll do positive and negative\n",
        "vector_index = 10 # any number 0-511\n",
        "\n",
        "current_eigvec = eigvec[vector_index]\n",
        "direction = degree * current_eigvec\n",
        "\n",
        "z0 = z - direction # move z in negative direction\n",
        "z1 = z # just z\n",
        "z2 = z + direction # move z in positive direction\n",
        "\n",
        "zs = torch.cat((z0,z1,z2)).to(device)\n",
        "# print(zs.shape)\n",
        "\n",
        "img = G(zs, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png') # uncomment to save images\n",
        "\n",
        "imshow(img.cpu(),col=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w4Nk0myytV4"
      },
      "source": [
        "seed = 2991\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "z = torch.from_numpy(z)\n",
        "\n",
        "degree = 10.0 # we'll do positive and negative\n",
        "vector_index = 100\n",
        "\n",
        "current_eigvec = eigvec[vector_index]\n",
        "direction = degree * current_eigvec\n",
        "\n",
        "z = z.cpu()\n",
        "\n",
        "z0 = z - direction # move z in negative direction\n",
        "z1 = z - (direction/2) # move z in negative direction (half way)\n",
        "z2 = z # just z\n",
        "z3 = z + (direction/2) # move z in positive direction\n",
        "z4 = z + direction # move z in positive direction\n",
        "\n",
        "zs = torch.cat((z0,z1,z2,z3,z4)).to(device)\n",
        "\n",
        "img = G(zs, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "# PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png') # uncomment to save images\n",
        "\n",
        "imshow(img.cpu(),col=5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vLCCxDC1mpO"
      },
      "source": [
        "What if we looked at just the poles and interpolated in the w space?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_fPC8gD2IWc"
      },
      "source": [
        "def lerp(zs, steps):\n",
        "    out = []\n",
        "    for i in range(len(zs)-1):\n",
        "        for index in range(steps):\n",
        "            t = index/float(steps)\n",
        "            out.append(zs[i+1]*t + zs[i]*(1-t))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMABcdmThgVu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6phDZXV30Eyc"
      },
      "source": [
        "zs = [z0,z4]\n",
        "ws = []\n",
        "\n",
        "for z_idx, z in enumerate(zs):\n",
        "    z = z.to(device)\n",
        "    w = G.mapping(z, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n",
        "    ws.append(w)\n",
        "\n",
        "frame_ws = lerp(ws, 5)\n",
        "\n",
        "ws = torch.cat((frame_ws[0], frame_ws[1], frame_ws[2], frame_ws[3], frame_ws[4]))\n",
        "\n",
        "img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=True)\n",
        "img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "imshow(img.cpu(),col=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FMDbv8RgHD0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRZHhbvm4i4Z"
      },
      "source": [
        "For homework/fun, maybe convert thes to videos based on what we’ve done earlier in this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OezvCXYJ2Ezt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}