{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2-LatentVectors.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipecalegario/stylegan2-ada-experiments/blob/main/StyleGAN2_LatentVectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RxzI5LeWPh"
      },
      "source": [
        "# Advanced StyleGAN Week 3: Latent Vectors\n",
        "\n",
        "Pretty much everything we’re going to do in the next couple of weeks will be about manipulating vectors. So in order to get a better sense of what we’re doing we should better understand exactly what a vector is, how to manipulat them, and the difference between a Z and W vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaMKANd0ePGY"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "!pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mX75wLlvAkz"
      },
      "source": [
        "!pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ZwnwAxhouT"
      },
      "source": [
        "Let’s also download a StyleGAN model file. You can import your own, or there are many to pick on the [Awesome StyleGAN2 Pretrained Model page](https://github.com/justinpinkney/awesome-pretrained-stylegan2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-EYt0Iuu6W"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAUuv2mKhSh6"
      },
      "source": [
        "!wget http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-cat-config-f.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYj2nDVHH0Z8"
      },
      "source": [
        "# %cd ../\n",
        "%mkdir dvschultz\n",
        "%cd dvschultz\n",
        "!git clone https://github.com/dvschultz/stylegan2-ada-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3FY3IzCvMMH"
      },
      "source": [
        "%cd stylegan2-ada-pytorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-uOyNKsf8sw"
      },
      "source": [
        "Set the path to your .pkl file below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id7FJSJxf1bR"
      },
      "source": [
        "Let’s import some libraries (some are python libraries, others are from the StyleGAN repo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VHkwrNQfeY5"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import dnnlib\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "\n",
        "import legacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz9szopvfM61"
      },
      "source": [
        "network_pkl = '/content/drive/MyDrive/GDL-studies/ERN-FUFI/modelo/ernesto-inicia-network-snapshot-000016.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3GWdiYYetmQ"
      },
      "source": [
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "\n",
        "device = torch.device('cuda') # we will use a GPU\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXwmnIgngv8h"
      },
      "source": [
        "## Generating images from a Z space Vector\n",
        "\n",
        "Now that we’ve loaded our model, we can generate a random vector.\n",
        "\n",
        "`seeds`, as used in the StyleGAN model, refer to a random seed value. This allows us to generate the same random values every time as long as the seed value is the same.\n",
        "\n",
        "`G.z_dim` in most cases is 512 (This can be customized, hence why we pull it directly from the model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IM1Xu3bg-hu"
      },
      "source": [
        "seed = 3923\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim) \n",
        "\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpIkZds6jA0f"
      },
      "source": [
        "Next, we’ll load this vector into PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOvtuvlQi-KL"
      },
      "source": [
        "z = torch.from_numpy(z).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7RW204QjMgL"
      },
      "source": [
        "Now we can generate an image from the vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BBxQ0C-h_qY"
      },
      "source": [
        "truncation_psi = 1\n",
        "noise_mode = 'const' # 'const', 'random', 'none'\n",
        "outdir = '/content/drive/MyDrive/GDL-studies/ERN-FUFI/02-09-2021'\n",
        "\n",
        "# make sure our output directory exists\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "# label is for class-based models. Let's assume we're not doing that here.\n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "\n",
        "img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "print(img)\n",
        "img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "print(img)\n",
        "PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}_1_0_random.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5DZ829IiswL"
      },
      "source": [
        "## Linear Interpolation\n",
        "\n",
        "Let’s look at how to interpolate between zs.\n",
        "\n",
        "We’ll start by defining a lerp function.\n",
        "\n",
        "`z[0]*t + z[1]*(1-t)` where t is time (or steps between each z)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrFYDKHXj5fT"
      },
      "source": [
        "def lerp(zs, steps):\n",
        "    out = []\n",
        "    for i in range(len(zs)-1):\n",
        "        for index in range(steps):\n",
        "            t = index/float(steps)\n",
        "            out.append(zs[i+1]*t + zs[i]*(1-t))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYhBx6etkGLy"
      },
      "source": [
        "Now let’s create two z vectors, then create the lerp vectors, then render them all as images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcZ7VSYVkCLf"
      },
      "source": [
        "z1 = np.random.RandomState(20).randn(1, G.z_dim)\n",
        "z2 = np.random.RandomState(100).randn(1, G.z_dim)\n",
        "\n",
        "frame_zs = lerp([z1,z2], 72)\n",
        "\n",
        "print('how many lerp frames? ',len(frame_zs))\n",
        "\n",
        "outdir = '/content/output-frames/'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "# label is still 0\n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "\n",
        "for idx, z in enumerate(frame_zs):\n",
        "    z = torch.from_numpy(z).to(device)\n",
        "    print('Generating frame %d/%d' % (idx, len(frame_zs)))\n",
        "    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/frame-{idx:04d}.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59WbUZQ7lVuj"
      },
      "source": [
        "And finally let’s convert it to a video using ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGYMRhqskcU_"
      },
      "source": [
        "!ffmpeg -i /content/output-frames/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/lerp.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJ0SBNsmHLN"
      },
      "source": [
        "## W space interpolation\n",
        "\n",
        "W space produced less entangled interpolations. A couple notes about the W space:\n",
        "\n",
        "* The process is to take a Z vector, project it to the W space, and then interpolate in the W space.\n",
        "* Interpolating in Z and then converting to W won’t do much. That’s because a specific vector in Z and in W should look exactly the same.\n",
        "* You will often start with a Z vector and project it to the W. I can’t see a reason why you would do the opposite (maybe there’s some reason but it would be an edge case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ4dQpeRnNlr"
      },
      "source": [
        "So let’s start by making two Z vectors and then converting them to two W vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVFc9wurl7kM"
      },
      "source": [
        "z1 = np.random.RandomState(20).randn(1, G.z_dim)\n",
        "z2 = np.random.RandomState(100).randn(1, G.z_dim)\n",
        "\n",
        "zs = [z1,z2]\n",
        "\n",
        "ws = []\n",
        "for z_idx, z in enumerate(zs):\n",
        "    z = torch.from_numpy(z).to(device)\n",
        "    w = G.mapping(z, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n",
        "    ws.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zej-5LhfnmCf"
      },
      "source": [
        "If a Z vector is 512 dimensions (often shown as `[1, 512]`) then a W vector is multiple \"stacks\" of 512 dimensions. The number of stacks is often dependent on the resolution of the model (it’s also settable in the training config).\n",
        "\n",
        "If you used the cat model that I do in this demo you find it has a shape of `[1, 14, 512]`. A 1024x1024 model is usually `[1, 18, 512]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABreV-T3nedb"
      },
      "source": [
        "print(ws[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wryr68HQoWy9"
      },
      "source": [
        "The lerp code is actually the exact same (thanks numpy!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLv6veNroBUp"
      },
      "source": [
        "frame_ws = lerp(ws, 100)\n",
        "\n",
        "print(len(frame_ws))\n",
        "\n",
        "print(frame_ws[49].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTSj-9lapPHI"
      },
      "source": [
        "And now we can generate images by using the `G.synthesis` network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ratxn3MovrK"
      },
      "source": [
        "outdir = '/content/output-frames-w/'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "for idx, w in enumerate(frame_ws): \n",
        "    img = G.synthesis(w, noise_mode=noise_mode, force_fp32=True)\n",
        "    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/frame-{idx:04d}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ETu0bPvp3ek"
      },
      "source": [
        "!ffmpeg -i /content/output-frames-w/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/lerp-w.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0XgSZGXqNDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}